# ChaosSense ðŸ”¥  
*A Real-Time AI Engine for Pattern Discovery in System-Level Chaos*

ChaosSense is an experimental backend-only project that listens to **chaotic system signals** (CPU usage, RAM usage, and network latency/ping jitter) and tries to extract **hidden patterns, anomalies, and "emerging order"** using streaming pipelines and machine learning.

This is my **first AWS project**, designed to help me learn:

- AWS streaming (Kinesis)
- Data lakes (S3)
- Basic ETL (Glue-style jobs)
- Simple ML models on time-series (PyTorch/TF or Scikit-learn)
- Serverless APIs (Lambda + API Gateway)

---

## ðŸŽ¯ Project Goals

- Collect high-frequency **CPU, RAM, and network jitter** from a system.
- Stream that data to AWS using **Kinesis Data Streams**.
- Store it in an **S3-based data lake** for analysis.
- Run **batch jobs** to compute chaos features like:
  - entropy
  - volatility
  - autocorrelation
- Train simple models to:
  - forecast short-term behavior
  - detect anomalies
- Expose **backend APIs** like `/chaos/forecast` and `/chaos/insights`.

No frontend.  
This is 100% **backend + data + ML + AWS**.

---

## ðŸ§± High-Level Architecture (v1)

**Version 1 focuses on:**

1. **Local collector**
   - Python script that reads:
     - CPU usage (%)
     - RAM usage (%)
     - Network latency (ping a known host)
   - Emits events like:

     ```json
     {
       "timestamp": "2025-01-01T12:00:00Z",
       "host_id": "local-machine-1",
       "cpu_percent": 37.2,
       "ram_percent": 61.4,
       "ping_ms": 23.7
     }
     ```

2. **Streaming to AWS (Kinesis)**
   - Collector sends each event to a **Kinesis Data Stream**.
   - Partition key examples: `"cpu"`, `"ram"`, `"network"` or `"host_id"`.

3. **Data Lake (S3)**
   - Downstream consumer (or Kinesis Firehose later) writes streaming data to S3.
   - Folder layout (example):

     ```
     s3://chaossense-data/
       chaos-raw/year=2025/month=01/day=01/...
     ```

4. **Batch Analysis (Local or Glue-style)**
   - Use a Jupyter notebook to:
     - load data from S3 (or local file during early stages)
     - compute rolling averages, entropy, volatility
     - visualize CPU/RAM/latency time-series

5. **Basic ML (Offline First)**
   - Use Python notebook to:
     - train a simple forecasting model (e.g. LSTM, ARIMA, or even baseline models)
     - train an anomaly detector (e.g. IsolationForest)
   - Save results and notes inside `ml/` folder.

6. **APIs (Later in v1.5 / v2)**
   - Wrap minimal inference logic in **AWS Lambda**.
   - Expose via **API Gateway** as:
     - `GET /chaos/forecast`
     - `GET /chaos/insights`

---

## ðŸ§© Tech Stack

- **Language:** Python 3.x
- **Local Metrics:** `psutil`, `subprocess` (for ping)
- **Core AWS Services (v1):**
  - Amazon Kinesis Data Streams
  - Amazon S3
- **Optional (later versions):**
  - AWS Glue / Glue ETL scripts
  - Amazon SageMaker (for training)
  - AWS Lambda + API Gateway
  - Terraform or AWS CDK for infrastructure

---

## ðŸ“‚ Repository Structure

```bash
chaossense/
â”œâ”€ README.md
â”œâ”€ requirements.txt              # Python deps for local + Lambda code
â”œâ”€ config/
â”‚  â”œâ”€ config.example.yaml        # Sample config for streams, regions, etc.
â”‚  â””â”€ logging.conf               # Optional logging configuration
â”œâ”€ collectors/                   # Local/system agents that collect chaos signals
â”‚  â”œâ”€ cpu_ram_network_agent.py   # Collect CPU, RAM, ping jitter & send to Kinesis
â”‚  â””â”€ utils_system_metrics.py    # Helper funcs (psutil, ping, etc.)
â”œâ”€ streaming/                    # Real-time data pipeline components
â”‚  â”œâ”€ kinesis_producer/          # Simple producer logic (if separate from collector)
â”‚  â”‚  â””â”€ producer.py
â”‚  â””â”€ kinesis_consumer/          # For local debugging / testing consumers
â”‚     â””â”€ consumer.py
â”œâ”€ data_lake/                    # S3-related logic and schemas
â”‚  â”œâ”€ schemas/
â”‚  â”‚  â””â”€ chaos_event_schema.json # JSON schema for each event
â”‚  â””â”€ examples/
â”‚     â””â”€ sample_events.json      # Example of raw events
â”œâ”€ batch/                        # Batch/ETL jobs
â”‚  â”œâ”€ glue_jobs/
â”‚  â”‚  â””â”€ chaos_feature_etl.py    # Glue-style ETL script (can run locally first)
â”‚  â””â”€ notebooks/
â”‚     â””â”€ chaos_exploration.ipynb # Jupyter: entropy, plots, first analysis
â”œâ”€ ml/
â”‚  â”œâ”€ feature_engineering/
â”‚  â”‚  â””â”€ chaos_features.py       # Functions to compute entropy, etc.
â”‚  â”œâ”€ models/
â”‚  â”‚  â”œâ”€ forecasting.py          # Simple LSTM / baseline forecasting
â”‚  â”‚  â””â”€ anomaly_detection.py    # IsolationForest / RandomCutForest (offline first)
â”‚  â””â”€ experiments/
â”‚     â””â”€ chaos_signatures.ipynb  # Notebooks for experimenting
â”œâ”€ api/
â”‚  â”œâ”€ lambda/
â”‚  â”‚  â”œâ”€ chaos_forecast_handler.py   # Lambda entrypoint for /chaos/forecast
â”‚  â”‚  â””â”€ chaos_insights_handler.py   # Lambda entrypoint for /chaos/insights
â”‚  â””â”€ openapi/
â”‚     â””â”€ chaosense_api.yaml      # Optional: API spec for API Gateway
â”œâ”€ infra/                        # AWS infrastructure as code (add gradually)
â”‚  â”œâ”€ terraform/                 # OR use cdk/ if you prefer CDK
â”‚  â”‚  â””â”€ main.tf                 # Define Kinesis, S3, IAM, Lambda later
â”‚  â””â”€ diagrams/
â”‚     â””â”€ architecture.png        # Exported architecture diagram (optional)
â”œâ”€ scripts/
â”‚  â”œâ”€ setup_venv.sh              # Shell script to create venv and install deps
â”‚  â””â”€ local_run_demo.sh          # Run collector locally & write to file or stdout
â””â”€ docs/
   â”œâ”€ architecture.md            # High-level description of architecture
   â”œâ”€ milestones.md              # Roadmap v1, v2, v3
   â””â”€ chaos_metrics.md           # Notes on entropy, Lyapunov, etc.

