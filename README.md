# ChaosSense ðŸ”¥  
*A Real-Time AI Engine for Pattern Discovery in System-Level Chaos*

ChaosSense is an experimental backend-only project that listens to **chaotic system signals** (CPU usage, RAM usage, and network latency/ping jitter) and tries to extract **hidden patterns, anomalies, and "emerging order"** using streaming pipelines and machine learning.

This is my **first AWS project**, designed to help me learn:

- AWS streaming (Kinesis)
- Data lakes (S3)
- Basic ETL (Glue-style jobs)
- Simple ML models on time-series (PyTorch/TF or Scikit-learn)
- Serverless APIs (Lambda + API Gateway)

---

## ðŸŽ¯ Project Goals

- Collect high-frequency **CPU, RAM, and network jitter** from a system.
- Stream that data to AWS using **Kinesis Data Streams**.
- Store it in an **S3-based data lake** for analysis.
- Run **batch jobs** to compute chaos features like:
  - entropy
  - volatility
  - autocorrelation
- Train simple models to:
  - forecast short-term behavior
  - detect anomalies
- Expose **backend APIs** like `/chaos/forecast` and `/chaos/insights`.

No frontend.  
This is 100% **backend + data + ML + AWS**.

---

## ðŸ§± High-Level Architecture (v1)

**Version 1 focuses on:**

1. **Local collector**
   - Python script that reads:
     - CPU usage (%)
     - RAM usage (%)
     - Network latency (ping a known host)
   - Emits events like:

     ```json
     {
       "timestamp": "2025-01-01T12:00:00Z",
       "host_id": "local-machine-1",
       "cpu_percent": 37.2,
       "ram_percent": 61.4,
       "ping_ms": 23.7
     }
     ```

2. **Streaming to AWS (Kinesis)**
   - Collector sends each event to a **Kinesis Data Stream**.
   - Partition key examples: `"cpu"`, `"ram"`, `"network"` or `"host_id"`.

3. **Data Lake (S3)**
   - Downstream consumer (or Kinesis Firehose later) writes streaming data to S3.
   - Folder layout (example):

     ```
     s3://chaossense-data/
       chaos-raw/year=2025/month=01/day=01/...
     ```

4. **Batch Analysis (Local or Glue-style)**
   - Use a Jupyter notebook to:
     - load data from S3 (or local file during early stages)
     - compute rolling averages, entropy, volatility
     - visualize CPU/RAM/latency time-series

5. **Basic ML (Offline First)**
   - Use Python notebook to:
     - train a simple forecasting model (e.g. LSTM, ARIMA, or even baseline models)
     - train an anomaly detector (e.g. IsolationForest)
   - Save results and notes inside `ml/` folder.

6. **APIs (Later in v1.5 / v2)**
   - Wrap minimal inference logic in **AWS Lambda**.
   - Expose via **API Gateway** as:
     - `GET /chaos/forecast`
     - `GET /chaos/insights`

---

## ðŸ§© Tech Stack

- **Language:** Python 3.x
- **Local Metrics:** `psutil`, `subprocess` (for ping)
- **Core AWS Services (v1):**
  - Amazon Kinesis Data Streams
  - Amazon S3
- **Optional (later versions):**
  - AWS Glue / Glue ETL scripts
  - Amazon SageMaker (for training)
  - AWS Lambda + API Gateway
  - Terraform or AWS CDK for infrastructure

---

## ðŸ“‚ Repository Structure

```bash
chaossense/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ config/
â”‚  â””â”€ config.example.yaml
â”œâ”€ collectors/
â”‚  â”œâ”€ cpu_ram_network_agent.py
â”‚  â””â”€ utils_system_metrics.py
â”œâ”€ streaming/
â”‚  â”œâ”€ kinesis_producer/producer.py
â”‚  â””â”€ kinesis_consumer/consumer.py
â”œâ”€ data_lake/
â”‚  â”œâ”€ schemas/chaos_event_schema.json
â”‚  â””â”€ examples/sample_events.json
â”œâ”€ batch/
â”‚  â”œâ”€ glue_jobs/chaos_feature_etl.py
â”‚  â””â”€ notebooks/chaos_exploration.ipynb
â”œâ”€ ml/
â”‚  â”œâ”€ feature_engineering/chaos_features.py
â”‚  â”œâ”€ models/forecasting.py
â”‚  â””â”€ experiments/chaos_signatures.ipynb
â”œâ”€ api/
â”‚  â”œâ”€ lambda/chaos_forecast_handler.py
â”‚  â””â”€ openapi/chaosense_api.yaml
â”œâ”€ infra/
â”‚  â”œâ”€ terraform/main.tf
â”‚  â””â”€ diagrams/architecture.png
â”œâ”€ scripts/
â”‚  â”œâ”€ setup_venv.sh
â”‚  â””â”€ local_run_demo.sh
â””â”€ docs/
   â”œâ”€ architecture.md
   â”œâ”€ milestones.md
   â””â”€ chaos_metrics.md
